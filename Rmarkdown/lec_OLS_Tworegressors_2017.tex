\documentclass[xcolor=dvipsnames]{beamer}
\usepackage{amsfonts, epsfig, xspace, relsize}
\usepackage{algorithm,algorithmic, graphicx}
\usepackage{pstricks,pst-node}
\usepackage{multimedia}
\usepackage{enumerate}
\usepackage[normal,tight,center]{subfigure}
\setlength{\subfigcapskip}{-.5em}
\usepackage{beamerthemesplit}
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{fancyvrb}
\usepackage{framed,color}
\definecolor{shadecolor}{rgb}{0.8,1,0.5}
\usepackage{amsmath}% http://ctan.org/pkg/amsmath
\DeclareMathOperator*{\plim}{plim}
\DeclareMathOperator*{\argmin}{argmin}
\usepackage[retainorgcmds]{IEEEtrantools}% http://ctan.org/pkg/ieeetran
\newcommand{\non}{\IEEEnonumber*}

\usepackage[font=small,labelfont=bf,tableposition=top]{caption}
%\usepackage{scalerel}
%\newcommand\reallywidehat[1]{%
%\begin{array}{c}
%\stretchto{
%  \scaleto{
%    \scalerel*[\widthof{#1}]{\bigwedge}
%    {\rule[-\textheight/2]{1ex}{\textheight}} %WIDTH-LIMITED BIG WEDGE
%  }{1.25\textheight} % THIS STRETCHES THE WEDGE A LITTLE EXTRA WIDE
%}{0.5ex}\\           % THIS SQUEEZES THE WEDGE TO 0.5ex HEIGHT
%#1\\                 % THIS STACKS THE WEDGE ATOP THE ARGUMENT
%\rule{-1ex}{0ex}
%\end{array}
%}
\setbeamertemplate{caption}[numbered]
\usepackage{fontspec}  %加這個就可以設定字體
\usepackage{xeCJK}       %讓中英文字體分開設置
%\setromanfont{LiHei Pro} % 儷黑Pro

\newCJKfontfamily{\K}{標楷體}
\newCJKfontfamily{\H}{微軟正黑體}
\setsansfont{Times New Roman}
\setmainfont{Times New Roman}
\setmonofont[Scale=0.8]{Courier New} % 等寬字型
\setCJKmainfont{王漢宗細圓體繁} %設定中文為系統上的字型，而英文不去更動，使用原TeX字型
\XeTeXlinebreaklocale "zh"             %這兩行一定要加，中文才能自動換行
\XeTeXlinebreakskip = 0pt plus 1pt     %這兩行一定要加，中文才能自動換行
%\usetheme{lankton-keynote}
\usetheme{Madrid}
\usecolortheme[named=BrickRed]{structure}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{filecontents}
\author[蔡佳泓]{\K 蔡佳泓}

\title[Statistical Methods for Social Sciences]{Regression Analysis\\
\smallskip
{\small {Multiple Linear Regression: Two Regressors}}}
\date[5/23/2017]{2017年5月23日} %leave out for today's date to be insterted
\institute[ESC \& GIEAS]{\H 國立政治大學選舉研究中心暨東亞研究所}
\begin{document}
\maketitle
\tableofcontents
\section{介紹}
\begin{frame}{\H 為何需要超過一個迴歸變數？}
複迴歸的用途：
\begin{enumerate}
\item 可以統計更多資訊，做為描述之用
\item 改善模型的預測能力，解釋單一變數無法解釋的變異量
\item 控制可能的「混淆」(confounding)變數，以利因果推論
\item 估計更複雜的模型(例如：$Y=\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}^2$)
\item 估計有交互作用變項的模型(例如：$Y=\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\beta_{3}X_{1}X_{2}$)
\end{enumerate}
\end{frame}
\begin{frame}{\H 參數、樣本統計以及樣本估計}
\begin{itemize}
\item \textcolor{blue}{參數}(parameter)：$E[Y|X_{1},\dotsc,X_{k}]$。
\item \textcolor{blue}{樣本統計}(estimator)：$\widehat{E}[Y|X_{1},\dotsc,X_{k}]$
\item \textcolor{blue}{樣本估計}(estimate)：特定樣本的$\widehat{E}[Y|X_{1},\dotsc,X_{k}]$
\end{itemize}
\end{frame}
\begin{frame}{母體模型}
\begin{itemize}
\item 單迴歸的模型寫成:
\[E[Y|X_{1}=x_{1}]=\beta_{0}+\beta_{1}x_{1}\]
\item 雙變數的迴歸模型可寫成:
\[E[Y|X_{1}=x_{1}, X_{2}=x_{2}]=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}\]

\end{itemize}
\end{frame}
\section{兩個自變數}
\subsection{一個連續變數、一個類別變數}
\begin{frame}{\H 實例：學童成績與母親教育程度、智商}
之前的模型：
\begin{center}
$\widehat{\mathrm {Kid's\hspace{.25em} Scores}}=\hat{\beta}_{0}+\hat{\beta}_{1}\mathrm {Mom's\hspace{.25em} IQ}$
\end{center}
現在的模型：
\begin{center}
$\widehat{\mathrm {Kid's\hspace{.25em} Scores}}=\hat{\beta}_{0}+\hat{\beta}_{1}(\mathrm {Mom\hspace{.25em} has\hspace{.25em} HS\hspace{.25em} degree})+\hat{\beta}_{2}(\mathrm {Mom's\hspace{.25em} IQ})$
\end{center}
\end{frame}
\begin{frame}{\H 分別估計模型}
 \begin{minipage}{\textwidth}
 \begin{minipage}[b]{0.49\textwidth}
    \centering
%    \rule{6.4cm}{3.6cm}
     \includegraphics[scale=.3]{kidscore1.1.jpg}
    \captionof{figure}{學童分數與母親有無高中畢業}
  \end{minipage}
  \hfill
 \begin{minipage}[b]{0.49\textwidth}
    \centering
   \begin{tabular}{|c c c|}
\hline
 & coef.est & coef.se \\

(Intercept) & 77.55 & 2.06  \\
mom$\_$hs & 11.77  &   2.32 \\
\hline
     \end{tabular}
   \captionof{table}{母親有無高中畢業迴歸模型估計}
  \end{minipage}
\end{minipage}
\end{frame} 

\begin{frame}
 \begin{minipage}{\textwidth}
 \begin{minipage}[b]{0.49\textwidth}
    \centering
%    \rule{6.4cm}{3.6cm}
     \includegraphics[scale=.3]{kidscore1.2.jpg}
    \captionof{figure}{學童分數與母親智商}
  \end{minipage}
  \hfill
 \begin{minipage}[b]{0.49\textwidth}
    \centering
   \begin{tabular}{|c c c|}
\hline
 & coef.est & coef.se \\

(Intercept) &  25.80   &  5.92  \\
mom$\_$iq &  0.61  &   0.06 \\
\hline
     \end{tabular}
   \captionof{table}{母親智商迴歸模型估計}
  \end{minipage}
\end{minipage}
\end{frame}  
\begin{frame}
\begin{center}
\begin{figure}
\includegraphics[scale=.45]{kidscore2.jpg}
\caption{雙變數迴歸之散佈圖}
\end{figure}
\end{center}
\end{frame}

\begin{frame}{\H 如何詮釋二元自變數的模型}
現在的模型：
\begin{center}
$\widehat{\mathrm {Kid's\hspace{.25em} Scores}}=\hat{\beta}_{0}+\hat{\beta}_{1}(\mathrm {Mom\hspace{.25em} has\hspace{.25em} HS\hspace{.25em} degree})+\hat{\beta}_{2}(\mathrm {Mom's\hspace{.25em} IQ})$
\end{center}
當$X_{1}=1$（母親有高中學歷），
\begin{align*}
\widehat{\mathrm {Kid's\hspace{.25em} Scores}} & =\hat{\beta}_{0}+\hat{\beta}_{1}\cdot 1+\hat{\beta}_{2}(\mathrm {Mom's\hspace{.25em} IQ})\\
& = \hat{\beta}_{0}+\hat{\beta}_{1}+\hat{\beta}_{2}(\mathrm {Mom's\hspace{.25em} IQ})
\end{align*}
當$X_{1}=0$（母親沒有高中學歷），
\begin{align*}
\widehat{\mathrm {Kid's\hspace{.25em} Scores}} & =\hat{\beta}_{0}+\hat{\beta}_{1}\cdot 0+\hat{\beta}_{2}(\mathrm {Mom's\hspace{.25em} IQ})\\
& = \hat{\beta}_{0}+\hat{\beta}_{2}(\mathrm {Mom's\hspace{.25em} IQ})
\end{align*}
也就是說，這兩個模型的斜率一樣，但是當$X_{1}=1$時，截距多了$\hat{\beta}_{1}$。
\end{frame}
\begin{frame}[fragile=singleslide]{\H 雙變數模型}
\begin{Verbatim}[frame=single,label=R code,
fontseries=b,xleftmargin=2mm,commandchars=\\\{\},
formatcom=\color{blue}]
Call:
lm(formula = kid_score ~ mom_iq + mom_hs)

Residuals:
    Min      1Q  Median      3Q     Max 
-52.873 -12.663   2.404  11.356  49.545 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 25.73154    5.87521   4.380 1.49e-05 ***
mom_hs       5.95012    2.21181   2.690  0.00742 ** 
mom_iq       0.56391    0.06057   9.309  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 18.14 on 431 degrees of freedom
Multiple R-squared:  0.2141,	Adjusted R-squared:  0.2105 
F-statistic: 58.72 on 2 and 431 DF,  p-value: < 2.2e-16
\end{Verbatim}
\end{frame}
\begin{frame}
估計得到的模型為：$\hat{Y}=25.7+5.95\cdot$ 高中畢業與否$+0.56\cdot $母親智商\\
係數分別代表的意義為：
\begin{itemize}
\item $\hat{\beta}_{0}$代表母親沒有高中畢業的迴歸線的截距
\item $\hat{\beta}_{1}$代表母親有無高中畢業的兩條迴歸線的垂直距離
\item $\hat{\beta}_{2}$代表兩條線的斜率
\end{itemize}
\end{frame}
\subsection{兩個連續變數}
\begin{frame}{\H 兩個連續變數}
改用兩個連續變數預測依變數，估計得到的模型為：$\hat{Y}=17.6+0.6\cdot $母親智商$+0.39\cdot $母親年齡\\
係數分別代表的意義為：
\begin{itemize}
\item $\hat{\beta}_{0}=17.6$代表母親智商與年齡等於0的截距（沒有意義）
\item $\hat{\beta}_{1}=0.6$代表對於同樣的兩個觀察值，\textcolor{blue}{母親年齡相同}但是母親智商有一單位的差異預期帶來的學生分數的差異。
\item $\hat{\beta}_{2}=0.39$代表對於同樣的兩個觀察值，\textcolor{blue}{母親智商相同}但是母親年齡有一單位的差異預期帶來的學生分數的差異。
\end{itemize}
因為
\begin{center}
$\mathlarger{\frac{\partial (y=\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2})}{\partial X_{1}}}=\beta_{1}$
\end{center}
所以$\beta_{1}$代表當其他變數在同一個值或是水準時，對於$y$的淨作用。
\end{frame}
\begin{frame}
\begin{center}
\begin{figure}
\includegraphics[scale=.42]{kid3d1.jpg}
\caption{三個變數之迴歸平面圖1(R package:{\tt rgl})}
\end{figure}
\end{center}
\end{frame}

\begin{frame}
\begin{center}
\begin{figure}
\includegraphics[scale=.45]{kid3d2.jpg}
\caption{三個變數之迴歸平面圖2(R package:{\tt {scatterplot3d}})}
\end{figure}
\end{center}
\end{frame}

\section{預測值}
\begin{frame}{\H 複迴歸的預測值及殘差}
\begin{itemize}
\item 預測值(fitted values): \\
$\hat{y}_{i}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{\mathrm{1} i}+\hat{\beta}_{2}x_{\mathrm{2} i}$對$i=1,\dotsc, n$
\item 殘差(residuals): \\
$\hat{u}_{i}=y_{i}-\hat{y}_{i}$對$i=1,\dotsc, n$
\item 殘差指的是觀察值到迴歸平面的垂直距離。
\end{itemize}
\end{frame}
\begin{frame}{\H 各項名詞的意義}
\begin{itemize}
\item $y_{i}-\bar{y}$等於是用y的平均值無法預測到$y_{i}$的部分
\item $\hat{u}_{i}=y_{i}-\hat{y}_{i}$是用迴歸線無法預測到$y_{i}$的部分
\item $\hat{y}-\bar{y}$是y的預測值與平均值的差異。
\end{itemize}
\end{frame}
\section{複迴歸的機制}
\subsection{變異數分析}
\begin{frame}{\H 各項名詞的意義}
\begin{itemize}
\item $\sum _{i}^n(y_{i}-\bar{y})^2$=SST=Var[y]\\
{\textcolor{red}{Total Sum of Squares}}\\
y的變異量(variability)。與樣本或是母體的變異數不同
\medskip
\item $\sum _{i}^n(\hat{y}_{i}-y_{i})^2$=$\sum _{i}^n\hat{u}^2$=SSR=Var[$\hat{u}$]\\
{\textcolor{red}{Residual Sum of Squares}}\\
\medskip
\item $\sum _{i}^n(\hat{y}-\bar{y})^2$=SSE=Var[$\hat{y}$]\\
{\textcolor{red}{Explained Sum of Squares}}\\
\medskip
\item SST=SSE+SSR
\item $r^2=\frac{SSE}{SST}$
\end{itemize}
\end{frame}
\begin{frame}{\H 調整後$r^2$}
\begin{itemize}
\item 自變數越多，$r^2$越大
\item 為了遵守模型簡潔的要求，調整後$r^2$考慮自變數的數目：
\begin{center}
Adjusted $r^2=1-(1-r^2)\frac{n-1}{n-k-1}$
\end{center}
\item 缺點是不再$0\leq $或$\leq 1$
\item 不能解釋為$y_{i}$的變異量被解釋的比例
\end{itemize}
\end{frame}

\subsection{最小平方法樣本統計}
\begin{frame}{\H 如何求出複迴歸的係數}
\begin{itemize}
\item 為自變數$x_{i},z_{i}$找出$(\hat{\beta}_{0},\hat{\beta}_{1},\hat{\beta}_{2}) $滿足以下的條件：
\end{itemize}
\begin{align*}
(\hat{\beta}_{0},\hat{\beta}_{1},\hat{\beta}_{2}) & =\argmin_{\hat{\beta}_{0},\hat{\beta}_{1},\hat{\beta}_{2}}\sum\limits_{i=1}^n \hat{u}_{i}^2  \\
& = \argmin_{\hat{\beta}_{0},\hat{\beta}_{1},\hat{\beta}_{2}}\sum\limits_{i=1}^n(y_{i}- \hat{y}_{i})^2  \\
& = \argmin_{\hat{\beta}_{0},\hat{\beta}_{1},\hat{\beta}_{2}}\sum\limits_{i=1}^n (y_{i}-\tilde{\beta}_{0}-x_{i}\tilde{\beta}_{1}-z_{i}\tilde{\beta}_{2})^2 
\end{align*}
\end{frame}
\begin{frame}
我們要最小化$ (\hat{\beta}_{0},\hat{\beta}_{1},\hat{\beta}_{2}) $，也就是以下的平方和：
\begin{center}
$S(\hat{\beta}_{0},\hat{\beta}_{1},\hat{\beta}_{2})  =\sum\limits_{i=1}^n (y_{i}-\tilde{\beta}_{0}-x_{i}\tilde{\beta}_{1}-z_{i}\tilde{\beta}_{2})^2 $
\end{center}
%\end{frame}
%\begin{frame}
首先對$\hat{\beta}_{0},\hat{\beta}_{1},\hat{\beta}_{2}$分別取微分
\begin{align*}
\frac{\partial S}{\partial \tilde{\beta}_{0}}& =\sum\limits_{i=1}^n(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}-\hat{\beta}_{2}z_{i})&=0\\
\frac{\partial S}{\partial \tilde{\beta}_{1}}&  =\sum\limits_{i=1}^nx_{i}(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}-\hat{\beta}_{2}z_{i})&=0\\
\frac{\partial S}{\partial\tilde{\beta}_{2}}& =\sum\limits_{i=1}^nz_{i}(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i}-\hat{\beta}_{2}z_{i})&=0
\end{align*}
設定三個方程式分別等於0，然後解($ \hat{\beta}_{0}, \hat{\beta}_{1},\hat{\beta}_{2} $)
\end{frame}
\begin{frame}
上面三個方程式要得到惟一解，必須有兩個條件成立：
\begin{itemize}
\item 觀察值多於自變數($n>2$)
\item $x$ 與 $z$互相成線性獨立，也就是
\begin{enumerate}
\item $x$, $z$不是常數
\item $x$ 不是 $z$的線性函數，反之亦然
\end{enumerate}
\end{itemize}
\end{frame}
\subsection{OLS係數}
\begin{frame}
OLS係數可表示如下：
\begin{align*}
\hat{\beta}_{0} & =\bar{y}-\hat{\beta}_{1}\bar{x}-\hat{\beta}_{2}\bar{z}\\
\hat{\beta}_{1} & = \mathlarger{\frac{Cov(x,y)Var(z)-Cov(z,y)Cov(x,z)}{Var(x)Var(z)-Cov(x,z)^2}}\\
\hat{\beta}_{2} & = \mathlarger{\frac{Cov(z,y)Var(x)-Cov(x,y)Cov(z,x)}{Var(x)Var(z)-Cov(x,z)^2}}
\end{align*}
($ \hat{\beta}_{0}, \hat{\beta}_{1},\hat{\beta}_{2} $)如果要被定義，以下條件必須成立：
\[Var(x)Var(z)\ne Cov(x,z)^2 \]
如果出現以下情形，則該條件不成立：
\begin{enumerate}
\item $x$或$z$是常數，也就是$Var(x)Var(z)=Cov(x,z)=0$
\item 一個自變數是另一個自變數的線性函數($\Rightarrow Cor(x,z)=1\Rightarrow Var(x)Var(z) = Cov(x,z)^2$)
\end{enumerate}
\end{frame}
\subsection{重要特性}
\begin{frame}{\H 重要特性}
與簡單OLS迴歸相同，複迴歸的樣本統計有以下的特性：
\begin{enumerate}
\item $\bar{\hat{u}}=0$
\begin{proof}
$\bar{\hat{u}}=\bar{y}-\hat{\beta}_{0}-\hat{\beta}_{1}\bar{x}-\hat{\beta}_{2}\bar{z}
= \bar{y}-(\bar{y}-\hat{\beta}_{1}\bar{x}-\hat{\beta}_{2}\bar{z})-
\hat{\beta}_{1}\bar{x}-\hat{\beta}_{2}\bar{z}=0$
\end{proof}
這個特性可導出$\bar{y} = \bar{\hat{y}}$

\begin{proof}
\[y_{i}=\hat{y}_{i}+\hat{u}_{i}\Rightarrow \bar{y}=n^{-1}\sum_{i=1}^{n}y_{i}=n^{-1}\sum_{i=1}^{n}(\hat{y}_{i}+\hat{u}_{i})=\bar{\hat{y}}-\bar{\hat{u}}=\bar{\hat{y}}\]
\end{proof}
\item $Cov(x,\hat{u})=Cov(z,\hat{u})=0$
可導出$Cov(\hat{y},\hat{u})=0$
\item ($\bar{x},\bar{y},\bar{z}$)一定會落在迴歸平面上。
\end{enumerate}
\end{frame}

\section{係數詮釋}
\begin{frame}{\H OLS係數的詮釋}
\begin{itemize}
\item 可以用兩個自變數預測彼此的殘差項解釋迴歸係數。
\item 假設\[Y=\beta_{0}+\beta_{1}X+\beta_{2}Z+u\] \\
估計的迴歸係數可寫成\[\hat{\beta}_{1}=\frac{\sum_{i}^{n}\hat{r}_{xz,i}y_{i}}{\sum_{i}^{n}\hat{r}_{xz,i}}\]
其中$\hat{r}_{xz,i}$代表用$Z$預測$X$的殘差：
\[X=\lambda+\delta\dot Z+r_{xz}\]
\item 也就是說，
\begin{itemize}
\item $\delta$代表$X,Z$的相關程度
\item $\hat{r}_{xz}$代表$X$與$Z$沒有相關的部份。
\item $\hat{r}_{xz}$是$X$被$Z$解釋過後的部份，也就是\textcolor{blue}{淨作用}
\item 所以$\hat{\beta}_{1}$代表$X$除去與$Z$相關部份之後的作用。
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[fragile=singleslide]{\H 原始模型}
\begin{Verbatim}[frame=single,label=R code,
fontseries=b,xleftmargin=2mm,commandchars=\\\{\},
formatcom=\color{blue}]

lm(formula = kid_score ~ mom_iq + mom_age)
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 17.59625    9.08397   1.937   0.0534 .  
\textcolor{red}{mom_iq       0.60357    0.05874}  10.275   <2e-16 ***
mom_age      0.38813    0.32620   1.190   0.2348   
\end{Verbatim}
\end{frame}

\begin{frame}[fragile=singleslide]{\H 淨相關}
\begin{Verbatim}[frame=single,label=R code,
fontseries=b,xleftmargin=2mm,commandchars=\\\{\},
formatcom=\color{blue}]

f1<-lm(mom_iq ~ mom_age)
iq＄res_momiq_momage<-residuals(f1)
summary(lm(kid_score ~ res_momiq_momage , data=iq))
Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)      86.79724    0.88000   98.63   <2e-16 ***
\textcolor{red}{res_momiq_momage  0.60357    0.05898 }  10.23   <2e-16 ***
\end{Verbatim}
\end{frame}

\begin{frame}
 \begin{minipage}{\textwidth}
 \begin{minipage}[b]{0.49\textwidth}
    \centering
%    \rule{6.4cm}{3.6cm}
     \includegraphics[scale=.3]{res1.jpg}
    \captionof{figure}{學童分數與母親智商殘差}
  \end{minipage}
  \hfill
 \begin{minipage}[b]{.5\textwidth}
    \centering
   \begin{tabular}{|l c |}
\hline
 & coef.est \\

(Intercept) &  80.80   \\
res$\_$momiq$\_$momage &  0.60   \\
\hline
     \end{tabular}
   \captionof{table}{淨相關之迴歸模型估計}
  \end{minipage}
\end{minipage}
\end{frame}  
\subsection{實例: 預期壽命}
\begin{frame}{實例: 預期壽命}
\begin{itemize}
\item 在\textbf{car}套件中，有一筆資料(\tt{UN11})，其中有預期壽命(\tt{lifeExpF})、經濟成長率(\tt{ppgdp})以及生育率(\tt{fertility})等三個變數。想要用經濟成長率$X_{1}$以及生育率$X_{2}$解釋壽命。
\item 其中經濟成長率取自然對數。
\item 分別畫$Y=\beta_{0}+\beta_{1}\textrm{log}(X_{1})$以及$Y=\beta_{0}+\beta_{2}X_{2}$的散佈圖以及迴歸線圖。
\item $e_{y}$與$e_{x2}$分別表示$Y$與$X_{2}$沒有被$X_{1}$解釋的部分。
\end{itemize}
\end{frame}
%UN11
\begin{frame}{$Y=\beta_{0}+\beta_{1}\textrm{log}(X_{1})$}
\begin{center}
\begin{tikzpicture}
\begin{axis}[ytick={50,60,70,80}, 
xlabel=log(ppgdp),
ylabel=lifeExpF,
legend cell align=left,
    legend pos=north west]
\addplot[only marks, mark=*, blue!80!black] table[row sep=\\]{
 X Y \\
6.212606096	49.49	\\
8.209906872	80.4	\\
8.405814603	75	\\
8.371450399	53.17	\\
9.528801376	81.1	\\
9.122830689	79.89	\\
8.016548895	77.33	\\
10.03677204	77.75	\\
10.95289034	84.27	\\
10.71794045	83.55	\\
8.637213722	73.66	\\
10.01956246	78.85	\\
9.808302865	76.06	\\
6.507874549	70.23	\\
9.581717704	80.26	\\
8.648572269	76.37	\\
10.68772694	82.81	\\
8.410898907	77.81	\\
6.608135569	58.66	\\
11.43631112	82.3	\\
7.624228285	69.84	\\
7.589790955	69.4	\\
8.406864801	78.4	\\
8.909627094	51.34	\\
9.279455903	77.41	\\
10.39352663	80.64	\\
8.758585222	77.12	\\
6.253251722	57.02	\\
5.173887288	52.58	\\
6.681105588	65.1	\\
7.095561766	53.56	\\
10.74421171	83.49	\\
8.084562415	77.7	\\
10.95164654	83.8	\\
6.111023782	51.3	\\
6.589476533	51.61	\\
9.383259531	82.35	\\
8.378850242	75.61	\\
8.735975245	77.69	\\
6.602045004	63.18	\\
7.887996859	59.33	\\
9.410182542	76.24546724	\\
8.949468993	81.99	\\
7.051076098	57.71	\\
9.533835917	80.37	\\
8.648993086	81.33	\\
10.25288659	82.14	\\
9.843673852	81	\\
5.301312876	50.56	\\
10.93007022	81.37	\\
7.156644547	60.04	\\
8.856632451	78.2	\\
8.555528898	76.57	\\
6.559756871	64.2	\\
8.312036895	78.91	\\
7.883710172	75.52	\\
8.139031918	77.09	\\
9.732248359	52.91	\\
6.061689992	64.41	\\
9.556437568	79.95	\\
5.782593655	61.59	\\
8.173490881	72.27	\\
10.70328267	83.28	\\
10.5852173	84.9	\\
10.11330267	78.07	\\
9.430984803	64.32	\\
6.361475174	60.3	\\
7.893684008	77.31	\\
10.59305584	82.99	\\
7.195337346	65.8	\\
10.1850434	82.58	\\
10.47143142	71.6	\\
8.913146539	77.72	\\
7.966343866	75.1	\\
6.057954288	56.39	\\
6.290457411	50.4	\\
8.005033345	73.45	\\
6.41787542	63.87	\\
7.613917397	75.92	\\
10.36796657	86.35	\\
9.46374151	78.47	\\
10.57841984	83.77	\\
7.248788527	67.62	\\
7.989323133	71.8	\\
8.56161191	75.28	\\
6.789534648	72.6	\\
10.74117437	83.17	\\
10.28573862	84.19	\\
10.43049455	84.62	\\
8.496786382	75.98	\\
10.67222678	87.12	\\
8.399602637	75.17	\\
9.123332631	72.84	\\
6.6868592	59.16	\\
7.29179244	63.1	\\
10.72393676	75.89	\\
6.763191828	72.36	\\
6.954257113	69.42	\\
9.274535084	78.51	\\
9.136015453	75.07	\\
6.888266602	48.11	\\
5.387243576	58.59	\\
9.334397021	77.86	\\
9.303420795	78.28	\\
11.56262379	82.67	\\
10.81958227	83.8	\\
6.044768319	68.61	\\
5.878855603	55.17	\\
9.032743636	76.86	\\
8.452014465	78.7	\\
6.394927653	53.14	\\
9.883244028	82.29	\\
8.029237382	70.6	\\
7.03094589	60.95	\\
8.921097081	76.89	\\
9.116106613	79.64	\\
7.892900206	70.17	\\
7.393755281	73.48	\\
7.717217752	72.83	\\
8.781064013	77.37	\\
7.960323629	74.86	\\
6.010040933	51.81	\\
6.775594375	67.87	\\
8.541827266	63.04	\\
8.730706521	57.1	\\
6.281705842	70.05	\\
9.919415034	79.86	\\
10.75597976	82.79	\\
10.4721905	80.49	\\
10.38505222	82.77	\\
7.031652916	77.45	\\
5.879694646	55.77	\\
7.122705355	53.38	\\
6.222576268	72.12	\\
11.34555597	83.47	\\
9.94227548	76.44	\\
6.91095017	66.88	\\
9.289317897	72.1	\\
7.506317017	74.81	\\
8.937743937	79.07	\\
7.264310216	65.52	\\
7.926999632	74.91	\\
8.596133753	76.9	\\
7.668607836	72.57	\\
9.414358187	80.56	\\
9.972901669	82.76	\\
10.18342723	83.2	\\
11.18993257	78.24	\\
9.954760347	83.95	\\
8.925640515	77.95	\\
9.244877055	75.01	\\
6.27720724	57.13	\\
8.806439041	77.54	\\
8.114713622	76.02	\\
7.157190164	66.48	\\
9.670034793	75.57	\\
6.939932011	60.92	\\
8.541534523	77.05	\\
9.345797409	78	\\
5.862778539	48.87	\\
10.68700318	83.71	\\
9.678842875	79.53	\\
10.04801205	82.84	\\
7.084645446	70	\\
4.743191484	53.38	\\
8.889418598	54.09	\\
10.32688426	84.76	\\
7.772879024	78.4	\\
8.727729606	74.73	\\
7.50928047	63.82	\\
8.856233556	74.18	\\
8.10506594	48.54	\\
10.79765946	83.65	\\
11.14012404	84.71	\\
7.983269516	77.72	\\
6.704414355	71.23	\\
6.246106765	60.31	\\
8.397170149	77.14	\\
8.436590327	77.76	\\
6.262636067	59.4	\\
8.172757329	75.38	\\
9.629386177	73.82	\\
8.348087914	77.05	\\
9.219805437	76.61	\\
8.431090492	69.4	\\
8.066898067	65.1	\\
6.232448017	55.44	\\
8.017966703	74.58	\\
10.58720794	78.02	\\
10.50031104	82.42	\\
10.7481942	81.31	\\
9.388687374	80.66	\\
7.263539827	71.9	\\
7.994126281	73.58	\\
9.510644944	77.73	\\
7.075555239	77.44	\\
7.270452055	67.66	\\
7.121090889	50.04	\\
6.351060222	52.72	\\
 };
  %\addplot[only marks, mark=*, red!80!black]table {\forbes};
  \addlegendentry{data}
 \addplot[no markers, very thick, red] table [row sep=\\,
 y={create col/linear regression={y=Y}}]{
 X Y \\
6.212606096	49.49	\\
8.209906872	80.4	\\
8.405814603	75	\\
8.371450399	53.17	\\
9.528801376	81.1	\\
9.122830689	79.89	\\
8.016548895	77.33	\\
10.03677204	77.75	\\
10.95289034	84.27	\\
10.71794045	83.55	\\
8.637213722	73.66	\\
10.01956246	78.85	\\
9.808302865	76.06	\\
6.507874549	70.23	\\
9.581717704	80.26	\\
8.648572269	76.37	\\
10.68772694	82.81	\\
8.410898907	77.81	\\
6.608135569	58.66	\\
11.43631112	82.3	\\
7.624228285	69.84	\\
7.589790955	69.4	\\
8.406864801	78.4	\\
8.909627094	51.34	\\
9.279455903	77.41	\\
10.39352663	80.64	\\
8.758585222	77.12	\\
6.253251722	57.02	\\
5.173887288	52.58	\\
6.681105588	65.1	\\
7.095561766	53.56	\\
10.74421171	83.49	\\
8.084562415	77.7	\\
10.95164654	83.8	\\
6.111023782	51.3	\\
6.589476533	51.61	\\
9.383259531	82.35	\\
8.378850242	75.61	\\
8.735975245	77.69	\\
6.602045004	63.18	\\
7.887996859	59.33	\\
9.410182542	76.24546724	\\
8.949468993	81.99	\\
7.051076098	57.71	\\
9.533835917	80.37	\\
8.648993086	81.33	\\
10.25288659	82.14	\\
9.843673852	81	\\
5.301312876	50.56	\\
10.93007022	81.37	\\
7.156644547	60.04	\\
8.856632451	78.2	\\
8.555528898	76.57	\\
6.559756871	64.2	\\
8.312036895	78.91	\\
7.883710172	75.52	\\
8.139031918	77.09	\\
9.732248359	52.91	\\
6.061689992	64.41	\\
9.556437568	79.95	\\
5.782593655	61.59	\\
8.173490881	72.27	\\
10.70328267	83.28	\\
10.5852173	84.9	\\
10.11330267	78.07	\\
9.430984803	64.32	\\
6.361475174	60.3	\\
7.893684008	77.31	\\
10.59305584	82.99	\\
7.195337346	65.8	\\
10.1850434	82.58	\\
10.47143142	71.6	\\
8.913146539	77.72	\\
7.966343866	75.1	\\
6.057954288	56.39	\\
6.290457411	50.4	\\
8.005033345	73.45	\\
6.41787542	63.87	\\
7.613917397	75.92	\\
10.36796657	86.35	\\
9.46374151	78.47	\\
10.57841984	83.77	\\
7.248788527	67.62	\\
7.989323133	71.8	\\
8.56161191	75.28	\\
6.789534648	72.6	\\
10.74117437	83.17	\\
10.28573862	84.19	\\
10.43049455	84.62	\\
8.496786382	75.98	\\
10.67222678	87.12	\\
8.399602637	75.17	\\
9.123332631	72.84	\\
6.6868592	59.16	\\
7.29179244	63.1	\\
10.72393676	75.89	\\
6.763191828	72.36	\\
6.954257113	69.42	\\
9.274535084	78.51	\\
9.136015453	75.07	\\
6.888266602	48.11	\\
5.387243576	58.59	\\
9.334397021	77.86	\\
9.303420795	78.28	\\
11.56262379	82.67	\\
10.81958227	83.8	\\
6.044768319	68.61	\\
5.878855603	55.17	\\
9.032743636	76.86	\\
8.452014465	78.7	\\
6.394927653	53.14	\\
9.883244028	82.29	\\
8.029237382	70.6	\\
7.03094589	60.95	\\
8.921097081	76.89	\\
9.116106613	79.64	\\
7.892900206	70.17	\\
7.393755281	73.48	\\
7.717217752	72.83	\\
8.781064013	77.37	\\
7.960323629	74.86	\\
6.010040933	51.81	\\
6.775594375	67.87	\\
8.541827266	63.04	\\
8.730706521	57.1	\\
6.281705842	70.05	\\
9.919415034	79.86	\\
10.75597976	82.79	\\
10.4721905	80.49	\\
10.38505222	82.77	\\
7.031652916	77.45	\\
5.879694646	55.77	\\
7.122705355	53.38	\\
6.222576268	72.12	\\
11.34555597	83.47	\\
9.94227548	76.44	\\
6.91095017	66.88	\\
9.289317897	72.1	\\
7.506317017	74.81	\\
8.937743937	79.07	\\
7.264310216	65.52	\\
7.926999632	74.91	\\
8.596133753	76.9	\\
7.668607836	72.57	\\
9.414358187	80.56	\\
9.972901669	82.76	\\
10.18342723	83.2	\\
11.18993257	78.24	\\
9.954760347	83.95	\\
8.925640515	77.95	\\
9.244877055	75.01	\\
6.27720724	57.13	\\
8.806439041	77.54	\\
8.114713622	76.02	\\
7.157190164	66.48	\\
9.670034793	75.57	\\
6.939932011	60.92	\\
8.541534523	77.05	\\
9.345797409	78	\\
5.862778539	48.87	\\
10.68700318	83.71	\\
9.678842875	79.53	\\
10.04801205	82.84	\\
7.084645446	70	\\
4.743191484	53.38	\\
8.889418598	54.09	\\
10.32688426	84.76	\\
7.772879024	78.4	\\
8.727729606	74.73	\\
7.50928047	63.82	\\
8.856233556	74.18	\\
8.10506594	48.54	\\
10.79765946	83.65	\\
11.14012404	84.71	\\
7.983269516	77.72	\\
6.704414355	71.23	\\
6.246106765	60.31	\\
8.397170149	77.14	\\
8.436590327	77.76	\\
6.262636067	59.4	\\
8.172757329	75.38	\\
9.629386177	73.82	\\
8.348087914	77.05	\\
9.219805437	76.61	\\
8.431090492	69.4	\\
8.066898067	65.1	\\
6.232448017	55.44	\\
8.017966703	74.58	\\
10.58720794	78.02	\\
10.50031104	82.42	\\
10.7481942	81.31	\\
9.388687374	80.66	\\
7.263539827	71.9	\\
7.994126281	73.58	\\
9.510644944	77.73	\\
7.075555239	77.44	\\
7.270452055	67.66	\\
7.121090889	50.04	\\
6.351060222	52.72	\\
 }; 
\addlegendentry{%
    $\pgfmathprintnumber{\pgfplotstableregressionb}
    \pgfmathprintnumber[print sign]{\pgfplotstableregressiona} \cdot x$} %
\end{axis}
\end{tikzpicture}
\end{center}
\end{frame}
\begin{frame}[fragile=singleslide]{R result of $Y=\beta_{0}+\beta_{1}e_{x2}$}
\begin{Verbatim}[frame=single,label=R code,
fontseries=b,xleftmargin=2mm,commandchars=\\\{\},
formatcom=\color{blue}]
> mx1<-lm (lifeExpF ~ log(ppgdp)); summary(mx1)

Call:
lm(formula = lifeExpF ~ log(ppgdp))

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  29.8148     2.5314   11.78   <2e-16 ***
log(ppgdp)    5.0188     0.2942   17.06   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 6.448 on 197 degrees of freedom
Multiple R-squared:  0.5964,	Adjusted R-squared:  0.5943 
\end{Verbatim}
\end{frame}
\begin{frame}{$Y=\beta_{0}+\beta_{2}X_{2}$}
\begin{center}
\begin{tikzpicture}
\begin{axis}[ytick={50,60,70,80}, 
xlabel=fertility,
ylabel=lifeExpF,
legend cell align=left,
    legend pos=north east]
\addplot[only marks, mark=*, blue!80!black] table[row sep=\\]{
 X Y \\
5.968	49.49	\\
1.525	80.4	\\
2.142	75	\\
5.135	53.17	\\
2	81.1	\\
2.172	79.89	\\
1.735	77.33	\\
1.671	77.75	\\
1.949	84.27	\\
1.346	83.55	\\
2.148	73.66	\\
1.877	78.85	\\
2.43	76.06	\\
2.157	70.23	\\
1.575	80.26	\\
1.479	76.37	\\
1.835	82.81	\\
2.679	77.81	\\
5.078	58.66	\\
1.76	82.3	\\
2.258	69.84	\\
3.229	69.4	\\
1.134	78.4	\\
2.617	51.34	\\
1.8	77.41	\\
1.984	80.64	\\
1.546	77.12	\\
5.75	57.02	\\
4.051	52.58	\\
2.422	65.1	\\
4.287	53.56	\\
1.691	83.49	\\
2.279	77.7	\\
1.6	83.8	\\
4.423	51.3	\\
5.737	51.61	\\
1.832	82.35	\\
1.559	75.61	\\
2.293	77.69	\\
4.742	63.18	\\
4.442	59.33	\\
2.530806284	76.24546724	\\
1.812	81.99	\\
4.224	57.71	\\
1.501	80.37	\\
1.451	81.33	\\
1.458	82.14	\\
1.501	81	\\
5.485	50.56	\\
1.885	81.37	\\
3.589	60.04	\\
3	78.2	\\
2.49	76.57	\\
5.918	64.2	\\
2.393	78.91	\\
2.636	75.52	\\
2.171	77.09	\\
4.98	52.91	\\
4.243	64.41	\\
1.702	79.95	\\
3.848	61.59	\\
2.602	72.27	\\
1.875	83.28	\\
1.987	84.9	\\
2.033	78.07	\\
3.195	64.32	\\
4.689	60.3	\\
1.528	77.31	\\
1.457	82.99	\\
3.988	65.8	\\
1.54	82.58	\\
2.217	71.6	\\
2.171	77.72	\\
3.84	75.1	\\
5.032	56.39	\\
4.877	50.4	\\
2.19	73.45	\\
3.159	63.87	\\
2.996	75.92	\\
1.137	86.35	\\
1.43	78.47	\\
2.098	83.77	\\
2.538	67.62	\\
2.055	71.8	\\
1.587	75.28	\\
4.535	72.6	\\
2.097	83.17	\\
2.909	84.19	\\
1.476	84.62	\\
2.262	75.98	\\
1.418	87.12	\\
2.889	75.17	\\
2.481	72.84	\\
4.623	59.16	\\
3.5	63.1	\\
2.251	75.89	\\
2.621	72.36	\\
2.543	69.42	\\
1.506	78.51	\\
1.764	75.07	\\
3.051	48.11	\\
5.038	58.59	\\
2.41	77.86	\\
1.495	78.28	\\
1.683	82.67	\\
1.163	83.8	\\
4.493	68.61	\\
5.968	55.17	\\
2.572	76.86	\\
1.668	78.7	\\
6.117	53.14	\\
1.284	82.29	\\
4.384466259	70.6	\\
4.361	60.95	\\
1.59	76.89	\\
2.227	79.64	\\
3.307	70.17	\\
1.45	73.48	\\
2.446	72.83	\\
1.63	77.37	\\
2.183	74.86	\\
4.713	51.81	\\
1.939	67.87	\\
3.055	63.04	\\
3.3	57.1	\\
2.587	70.05	\\
1.9	79.86	\\
1.794	82.79	\\
2.091	80.49	\\
2.135	82.77	\\
2.5	77.45	\\
6.925	55.77	\\
5.431	53.38	\\
1.988	72.12	\\
1.948	83.47	\\
2.146	76.44	\\
3.201	66.88	\\
2	72.1	\\
4.27	74.81	\\
2.409	79.07	\\
3.799	65.52	\\
2.858	74.91	\\
2.41	76.9	\\
3.05	72.57	\\
1.415	80.56	\\
1.312	82.76	\\
1.757	83.2	\\
2.204	78.24	\\
1.389	83.95	\\
1.428	77.95	\\
1.529	75.01	\\
5.282	57.13	\\
1.907	77.54	\\
3.763	76.02	\\
3.488	66.48	\\
2.639	75.57	\\
4.605	60.92	\\
1.562	77.05	\\
2.34	78	\\
4.728	48.87	\\
1.367	83.71	\\
1.372	79.53	\\
1.477	82.84	\\
4.041	70	\\
6.283	53.38	\\
2.383	54.09	\\
1.504	84.76	\\
2.235	78.4	\\
1.995	74.73	\\
4.225	63.82	\\
2.266	74.18	\\
3.174	48.54	\\
1.925	83.65	\\
1.536	84.71	\\
2.772	77.72	\\
3.162	71.23	\\
5.499	60.31	\\
1.397	77.14	\\
1.528	77.76	\\
3.864	59.4	\\
3.783	75.38	\\
1.632	73.82	\\
1.909	77.05	\\
2.022	76.61	\\
2.316	69.4	\\
3.7	65.1	\\
5.901	55.44	\\
1.483	74.58	\\
1.707	78.02	\\
1.867	82.42	\\
2.077	81.31	\\
2.043	80.66	\\
2.264	71.9	\\
3.75	73.58	\\
2.391	77.73	\\
1.75	77.44	\\
4.938	67.66	\\
6.3	50.04	\\
3.109	52.72	\\
 };
  %\addplot[only marks, mark=*, red!80!black]table {\forbes};
  \addlegendentry{data}
 \addplot[no markers, very thick, red] table [row sep=\\,
 y={create col/linear regression={y=Y}}]{
 X Y \\
5.968	49.49	\\
1.525	80.4	\\
2.142	75	\\
5.135	53.17	\\
2	81.1	\\
2.172	79.89	\\
1.735	77.33	\\
1.671	77.75	\\
1.949	84.27	\\
1.346	83.55	\\
2.148	73.66	\\
1.877	78.85	\\
2.43	76.06	\\
2.157	70.23	\\
1.575	80.26	\\
1.479	76.37	\\
1.835	82.81	\\
2.679	77.81	\\
5.078	58.66	\\
1.76	82.3	\\
2.258	69.84	\\
3.229	69.4	\\
1.134	78.4	\\
2.617	51.34	\\
1.8	77.41	\\
1.984	80.64	\\
1.546	77.12	\\
5.75	57.02	\\
4.051	52.58	\\
2.422	65.1	\\
4.287	53.56	\\
1.691	83.49	\\
2.279	77.7	\\
1.6	83.8	\\
4.423	51.3	\\
5.737	51.61	\\
1.832	82.35	\\
1.559	75.61	\\
2.293	77.69	\\
4.742	63.18	\\
4.442	59.33	\\
2.530806284	76.24546724	\\
1.812	81.99	\\
4.224	57.71	\\
1.501	80.37	\\
1.451	81.33	\\
1.458	82.14	\\
1.501	81	\\
5.485	50.56	\\
1.885	81.37	\\
3.589	60.04	\\
3	78.2	\\
2.49	76.57	\\
5.918	64.2	\\
2.393	78.91	\\
2.636	75.52	\\
2.171	77.09	\\
4.98	52.91	\\
4.243	64.41	\\
1.702	79.95	\\
3.848	61.59	\\
2.602	72.27	\\
1.875	83.28	\\
1.987	84.9	\\
2.033	78.07	\\
3.195	64.32	\\
4.689	60.3	\\
1.528	77.31	\\
1.457	82.99	\\
3.988	65.8	\\
1.54	82.58	\\
2.217	71.6	\\
2.171	77.72	\\
3.84	75.1	\\
5.032	56.39	\\
4.877	50.4	\\
2.19	73.45	\\
3.159	63.87	\\
2.996	75.92	\\
1.137	86.35	\\
1.43	78.47	\\
2.098	83.77	\\
2.538	67.62	\\
2.055	71.8	\\
1.587	75.28	\\
4.535	72.6	\\
2.097	83.17	\\
2.909	84.19	\\
1.476	84.62	\\
2.262	75.98	\\
1.418	87.12	\\
2.889	75.17	\\
2.481	72.84	\\
4.623	59.16	\\
3.5	63.1	\\
2.251	75.89	\\
2.621	72.36	\\
2.543	69.42	\\
1.506	78.51	\\
1.764	75.07	\\
3.051	48.11	\\
5.038	58.59	\\
2.41	77.86	\\
1.495	78.28	\\
1.683	82.67	\\
1.163	83.8	\\
4.493	68.61	\\
5.968	55.17	\\
2.572	76.86	\\
1.668	78.7	\\
6.117	53.14	\\
1.284	82.29	\\
4.384466259	70.6	\\
4.361	60.95	\\
1.59	76.89	\\
2.227	79.64	\\
3.307	70.17	\\
1.45	73.48	\\
2.446	72.83	\\
1.63	77.37	\\
2.183	74.86	\\
4.713	51.81	\\
1.939	67.87	\\
3.055	63.04	\\
3.3	57.1	\\
2.587	70.05	\\
1.9	79.86	\\
1.794	82.79	\\
2.091	80.49	\\
2.135	82.77	\\
2.5	77.45	\\
6.925	55.77	\\
5.431	53.38	\\
1.988	72.12	\\
1.948	83.47	\\
2.146	76.44	\\
3.201	66.88	\\
2	72.1	\\
4.27	74.81	\\
2.409	79.07	\\
3.799	65.52	\\
2.858	74.91	\\
2.41	76.9	\\
3.05	72.57	\\
1.415	80.56	\\
1.312	82.76	\\
1.757	83.2	\\
2.204	78.24	\\
1.389	83.95	\\
1.428	77.95	\\
1.529	75.01	\\
5.282	57.13	\\
1.907	77.54	\\
3.763	76.02	\\
3.488	66.48	\\
2.639	75.57	\\
4.605	60.92	\\
1.562	77.05	\\
2.34	78	\\
4.728	48.87	\\
1.367	83.71	\\
1.372	79.53	\\
1.477	82.84	\\
4.041	70	\\
6.283	53.38	\\
2.383	54.09	\\
1.504	84.76	\\
2.235	78.4	\\
1.995	74.73	\\
4.225	63.82	\\
2.266	74.18	\\
3.174	48.54	\\
1.925	83.65	\\
1.536	84.71	\\
2.772	77.72	\\
3.162	71.23	\\
5.499	60.31	\\
1.397	77.14	\\
1.528	77.76	\\
3.864	59.4	\\
3.783	75.38	\\
1.632	73.82	\\
1.909	77.05	\\
2.022	76.61	\\
2.316	69.4	\\
3.7	65.1	\\
5.901	55.44	\\
1.483	74.58	\\
1.707	78.02	\\
1.867	82.42	\\
2.077	81.31	\\
2.043	80.66	\\
2.264	71.9	\\
3.75	73.58	\\
2.391	77.73	\\
1.75	77.44	\\
4.938	67.66	\\
6.3	50.04	\\
3.109	52.72	\\
 }; 
\addlegendentry{%
    $\pgfmathprintnumber{\pgfplotstableregressionb}
    \pgfmathprintnumber[print sign]{\pgfplotstableregressiona} \cdot x$} %
\end{axis}
\end{tikzpicture}
\end{center}
\end{frame}

\subsection{Marginal plot}
\begin{frame}{$X_{2}=\beta_{0}+\beta_{1}\textrm{log}(X_{1})$}
\begin{center}
\begin{tikzpicture}
\begin{axis}[ytick={1,2,3,4,5,6,7}, 
xlabel=log(ppgdp),
ylabel=fertility,
legend cell align=left,
    legend pos=north east]
\addplot[only marks, mark=*, blue!80!black] table[row sep=\\]{
 X Y \\
6.212606096	5.968	\\
8.209906872	1.525	\\
8.405814603	2.142	\\
8.371450399	5.135	\\
9.528801376	2	\\
9.122830689	2.172	\\
8.016548895	1.735	\\
10.03677204	1.671	\\
10.95289034	1.949	\\
10.71794045	1.346	\\
8.637213722	2.148	\\
10.01956246	1.877	\\
9.808302865	2.43	\\
6.507874549	2.157	\\
9.581717704	1.575	\\
8.648572269	1.479	\\
10.68772694	1.835	\\
8.410898907	2.679	\\
6.608135569	5.078	\\
11.43631112	1.76	\\
7.624228285	2.258	\\
7.589790955	3.229	\\
8.406864801	1.134	\\
8.909627094	2.617	\\
9.279455903	1.8	\\
10.39352663	1.984	\\
8.758585222	1.546	\\
6.253251722	5.75	\\
5.173887288	4.051	\\
6.681105588	2.422	\\
7.095561766	4.287	\\
10.74421171	1.691	\\
8.084562415	2.279	\\
10.95164654	1.6	\\
6.111023782	4.423	\\
6.589476533	5.737	\\
9.383259531	1.832	\\
8.378850242	1.559	\\
8.735975245	2.293	\\
6.602045004	4.742	\\
7.887996859	4.442	\\
9.410182542	2.530806284	\\
8.949468993	1.812	\\
7.051076098	4.224	\\
9.533835917	1.501	\\
8.648993086	1.451	\\
10.25288659	1.458	\\
9.843673852	1.501	\\
5.301312876	5.485	\\
10.93007022	1.885	\\
7.156644547	3.589	\\
8.856632451	3	\\
8.555528898	2.49	\\
6.559756871	5.918	\\
8.312036895	2.393	\\
7.883710172	2.636	\\
8.139031918	2.171	\\
9.732248359	4.98	\\
6.061689992	4.243	\\
9.556437568	1.702	\\
5.782593655	3.848	\\
8.173490881	2.602	\\
10.70328267	1.875	\\
10.5852173	1.987	\\
10.11330267	2.033	\\
9.430984803	3.195	\\
6.361475174	4.689	\\
7.893684008	1.528	\\
10.59305584	1.457	\\
7.195337346	3.988	\\
10.1850434	1.54	\\
10.47143142	2.217	\\
8.913146539	2.171	\\
7.966343866	3.84	\\
6.057954288	5.032	\\
6.290457411	4.877	\\
8.005033345	2.19	\\
6.41787542	3.159	\\
7.613917397	2.996	\\
10.36796657	1.137	\\
9.46374151	1.43	\\
10.57841984	2.098	\\
7.248788527	2.538	\\
7.989323133	2.055	\\
8.56161191	1.587	\\
6.789534648	4.535	\\
10.74117437	2.097	\\
10.28573862	2.909	\\
10.43049455	1.476	\\
8.496786382	2.262	\\
10.67222678	1.418	\\
8.399602637	2.889	\\
9.123332631	2.481	\\
6.6868592	4.623	\\
7.29179244	3.5	\\
10.72393676	2.251	\\
6.763191828	2.621	\\
6.954257113	2.543	\\
9.274535084	1.506	\\
9.136015453	1.764	\\
6.888266602	3.051	\\
5.387243576	5.038	\\
9.334397021	2.41	\\
9.303420795	1.495	\\
11.56262379	1.683	\\
10.81958227	1.163	\\
6.044768319	4.493	\\
5.878855603	5.968	\\
9.032743636	2.572	\\
8.452014465	1.668	\\
6.394927653	6.117	\\
9.883244028	1.284	\\
8.029237382	4.384466259	\\
7.03094589	4.361	\\
8.921097081	1.59	\\
9.116106613	2.227	\\
7.892900206	3.307	\\
7.393755281	1.45	\\
7.717217752	2.446	\\
8.781064013	1.63	\\
7.960323629	2.183	\\
6.010040933	4.713	\\
6.775594375	1.939	\\
8.541827266	3.055	\\
8.730706521	3.3	\\
6.281705842	2.587	\\
9.919415034	1.9	\\
10.75597976	1.794	\\
10.4721905	2.091	\\
10.38505222	2.135	\\
7.031652916	2.5	\\
5.879694646	6.925	\\
7.122705355	5.431	\\
6.222576268	1.988	\\
11.34555597	1.948	\\
9.94227548	2.146	\\
6.91095017	3.201	\\
9.289317897	2	\\
7.506317017	4.27	\\
8.937743937	2.409	\\
7.264310216	3.799	\\
7.926999632	2.858	\\
8.596133753	2.41	\\
7.668607836	3.05	\\
9.414358187	1.415	\\
9.972901669	1.312	\\
10.18342723	1.757	\\
11.18993257	2.204	\\
9.954760347	1.389	\\
8.925640515	1.428	\\
9.244877055	1.529	\\
6.27720724	5.282	\\
8.806439041	1.907	\\
8.114713622	3.763	\\
7.157190164	3.488	\\
9.670034793	2.639	\\
6.939932011	4.605	\\
8.541534523	1.562	\\
9.345797409	2.34	\\
5.862778539	4.728	\\
10.68700318	1.367	\\
9.678842875	1.372	\\
10.04801205	1.477	\\
7.084645446	4.041	\\
4.743191484	6.283	\\
8.889418598	2.383	\\
10.32688426	1.504	\\
7.772879024	2.235	\\
8.727729606	1.995	\\
7.50928047	4.225	\\
8.856233556	2.266	\\
8.10506594	3.174	\\
10.79765946	1.925	\\
11.14012404	1.536	\\
7.983269516	2.772	\\
6.704414355	3.162	\\
6.246106765	5.499	\\
8.397170149	1.397	\\
8.436590327	1.528	\\
6.262636067	3.864	\\
8.172757329	3.783	\\
9.629386177	1.632	\\
8.348087914	1.909	\\
9.219805437	2.022	\\
8.431090492	2.316	\\
8.066898067	3.7	\\
6.232448017	5.901	\\
8.017966703	1.483	\\
10.58720794	1.707	\\
10.50031104	1.867	\\
10.7481942	2.077	\\
9.388687374	2.043	\\
7.263539827	2.264	\\
7.994126281	3.75	\\
9.510644944	2.391	\\
7.075555239	1.75	\\
7.270452055	4.938	\\
7.121090889	6.3	\\
6.351060222	3.109	\\
 };
  %\addplot[only marks, mark=*, red!80!black]table {\forbes};
  \addlegendentry{data}
 \addplot[no markers, very thick, red] table [row sep=\\,
 y={create col/linear regression={y=Y}}]{
 X Y \\
6.212606096	5.968	\\
8.209906872	1.525	\\
8.405814603	2.142	\\
8.371450399	5.135	\\
9.528801376	2	\\
9.122830689	2.172	\\
8.016548895	1.735	\\
10.03677204	1.671	\\
10.95289034	1.949	\\
10.71794045	1.346	\\
8.637213722	2.148	\\
10.01956246	1.877	\\
9.808302865	2.43	\\
6.507874549	2.157	\\
9.581717704	1.575	\\
8.648572269	1.479	\\
10.68772694	1.835	\\
8.410898907	2.679	\\
6.608135569	5.078	\\
11.43631112	1.76	\\
7.624228285	2.258	\\
7.589790955	3.229	\\
8.406864801	1.134	\\
8.909627094	2.617	\\
9.279455903	1.8	\\
10.39352663	1.984	\\
8.758585222	1.546	\\
6.253251722	5.75	\\
5.173887288	4.051	\\
6.681105588	2.422	\\
7.095561766	4.287	\\
10.74421171	1.691	\\
8.084562415	2.279	\\
10.95164654	1.6	\\
6.111023782	4.423	\\
6.589476533	5.737	\\
9.383259531	1.832	\\
8.378850242	1.559	\\
8.735975245	2.293	\\
6.602045004	4.742	\\
7.887996859	4.442	\\
9.410182542	2.530806284	\\
8.949468993	1.812	\\
7.051076098	4.224	\\
9.533835917	1.501	\\
8.648993086	1.451	\\
10.25288659	1.458	\\
9.843673852	1.501	\\
5.301312876	5.485	\\
10.93007022	1.885	\\
7.156644547	3.589	\\
8.856632451	3	\\
8.555528898	2.49	\\
6.559756871	5.918	\\
8.312036895	2.393	\\
7.883710172	2.636	\\
8.139031918	2.171	\\
9.732248359	4.98	\\
6.061689992	4.243	\\
9.556437568	1.702	\\
5.782593655	3.848	\\
8.173490881	2.602	\\
10.70328267	1.875	\\
10.5852173	1.987	\\
10.11330267	2.033	\\
9.430984803	3.195	\\
6.361475174	4.689	\\
7.893684008	1.528	\\
10.59305584	1.457	\\
7.195337346	3.988	\\
10.1850434	1.54	\\
10.47143142	2.217	\\
8.913146539	2.171	\\
7.966343866	3.84	\\
6.057954288	5.032	\\
6.290457411	4.877	\\
8.005033345	2.19	\\
6.41787542	3.159	\\
7.613917397	2.996	\\
10.36796657	1.137	\\
9.46374151	1.43	\\
10.57841984	2.098	\\
7.248788527	2.538	\\
7.989323133	2.055	\\
8.56161191	1.587	\\
6.789534648	4.535	\\
10.74117437	2.097	\\
10.28573862	2.909	\\
10.43049455	1.476	\\
8.496786382	2.262	\\
10.67222678	1.418	\\
8.399602637	2.889	\\
9.123332631	2.481	\\
6.6868592	4.623	\\
7.29179244	3.5	\\
10.72393676	2.251	\\
6.763191828	2.621	\\
6.954257113	2.543	\\
9.274535084	1.506	\\
9.136015453	1.764	\\
6.888266602	3.051	\\
5.387243576	5.038	\\
9.334397021	2.41	\\
9.303420795	1.495	\\
11.56262379	1.683	\\
10.81958227	1.163	\\
6.044768319	4.493	\\
5.878855603	5.968	\\
9.032743636	2.572	\\
8.452014465	1.668	\\
6.394927653	6.117	\\
9.883244028	1.284	\\
8.029237382	4.384466259	\\
7.03094589	4.361	\\
8.921097081	1.59	\\
9.116106613	2.227	\\
7.892900206	3.307	\\
7.393755281	1.45	\\
7.717217752	2.446	\\
8.781064013	1.63	\\
7.960323629	2.183	\\
6.010040933	4.713	\\
6.775594375	1.939	\\
8.541827266	3.055	\\
8.730706521	3.3	\\
6.281705842	2.587	\\
9.919415034	1.9	\\
10.75597976	1.794	\\
10.4721905	2.091	\\
10.38505222	2.135	\\
7.031652916	2.5	\\
5.879694646	6.925	\\
7.122705355	5.431	\\
6.222576268	1.988	\\
11.34555597	1.948	\\
9.94227548	2.146	\\
6.91095017	3.201	\\
9.289317897	2	\\
7.506317017	4.27	\\
8.937743937	2.409	\\
7.264310216	3.799	\\
7.926999632	2.858	\\
8.596133753	2.41	\\
7.668607836	3.05	\\
9.414358187	1.415	\\
9.972901669	1.312	\\
10.18342723	1.757	\\
11.18993257	2.204	\\
9.954760347	1.389	\\
8.925640515	1.428	\\
9.244877055	1.529	\\
6.27720724	5.282	\\
8.806439041	1.907	\\
8.114713622	3.763	\\
7.157190164	3.488	\\
9.670034793	2.639	\\
6.939932011	4.605	\\
8.541534523	1.562	\\
9.345797409	2.34	\\
5.862778539	4.728	\\
10.68700318	1.367	\\
9.678842875	1.372	\\
10.04801205	1.477	\\
7.084645446	4.041	\\
4.743191484	6.283	\\
8.889418598	2.383	\\
10.32688426	1.504	\\
7.772879024	2.235	\\
8.727729606	1.995	\\
7.50928047	4.225	\\
8.856233556	2.266	\\
8.10506594	3.174	\\
10.79765946	1.925	\\
11.14012404	1.536	\\
7.983269516	2.772	\\
6.704414355	3.162	\\
6.246106765	5.499	\\
8.397170149	1.397	\\
8.436590327	1.528	\\
6.262636067	3.864	\\
8.172757329	3.783	\\
9.629386177	1.632	\\
8.348087914	1.909	\\
9.219805437	2.022	\\
8.431090492	2.316	\\
8.066898067	3.7	\\
6.232448017	5.901	\\
8.017966703	1.483	\\
10.58720794	1.707	\\
10.50031104	1.867	\\
10.7481942	2.077	\\
9.388687374	2.043	\\
7.263539827	2.264	\\
7.994126281	3.75	\\
9.510644944	2.391	\\
7.075555239	1.75	\\
7.270452055	4.938	\\
7.121090889	6.3	\\
6.351060222	3.109	\\
 }; 
\addlegendentry{%
    $\pgfmathprintnumber{\pgfplotstableregressionb}
    \pgfmathprintnumber[print sign]{\pgfplotstableregressiona} \cdot x$} %
\end{axis}
\end{tikzpicture}
\end{center}
\end{frame}
\begin{frame}{分析結果}
\begin{itemize}
\item 用$X_{1}$以及$X_{2}$分別解釋$Y$，會分別解釋一部分的$Y$變異量，但是也可能有重疊的部分。
\item 因為$X_{1}$與$X_{2}$有線性關係，所以彼此之間有相關。
\item 因此，我們用$X_{1}$解釋$Y$並且存取殘差值$e_{y}$，代表沒有被$X_{1}$解釋的部分，然後用$X_{1}$解釋$X_{2}$並且存取殘差值$e_{X2}$，代表$X_{2}$與$X_{1}$沒有相關的部分。然後用$e_{X2}$解釋$e_{y}$，便是扣除$X_{1}$的影響之後，$X_{2}$解釋$Y$的變異量。
\end{itemize}
\end{frame}

\begin{frame}{$e_{y}=\beta_{0}+\beta_{1}\textrm{log}(e_{x2})$}
\begin{center}
\begin{tikzpicture}
\begin{axis}[ytick={}, 
xlabel=residual(fertility),
ylabel=residual(lifeExpF),
legend cell align=left,
    legend pos=north east]
\addplot[only marks, mark=*, blue!80!black] table[row sep=\\]{
 X Y \\
1.810684978	-11.50485012	\\
-1.393815488	9.381025966	\\
-0.655335721	2.997797302	\\
2.316355496	-18.65973441	\\
-0.100986622	3.461711494	\\
-0.180723623	4.289211563	\\
-1.303714187	7.281457834	\\
-0.115000798	-2.437709674	\\
0.731071923	-0.515556659	\\
-0.017617368	-0.056381809	\\
-0.505848218	0.496443205	\\
0.080327773	-1.251337644	\\
0.502328518	-2.981060484	\\
-1.817222998	7.753246102	\\
-0.493173914	2.35613315	\\
-1.167804934	3.149436525	\\
0.45264764	-0.644745189	\\
-0.115183012	5.782280021	\\
1.165947521	-4.319947457	\\
0.841834694	-4.911766024	\\
-1.023986957	1.760450414	\\
-0.074341086	1.493285706	\\
-1.662684507	6.392526534	\\
0.132071676	-23.19075468	\\
-0.45560242	1.023135391	\\
0.419217955	-1.33820223	\\
-1.032587372	3.347299621	\\
1.617888788	-4.178843831	\\
-0.750410681	-3.201691362	\\
-1.444804744	1.753828024	\\
0.677193996	-11.86625933	\\
0.343673092	-0.248232963	\\
-0.717539912	7.310109168	\\
0.381300663	-0.97931426	\\
0.202695142	-9.185025206	\\
1.813377302	-11.27630082	\\
-0.359235176	5.442162062	\\
-1.255055961	3.743126995	\\
-0.299607516	4.030775373	\\
0.826170843	0.230620086	\\
1.323572413	-10.07336065	\\
0.356265708	-0.797492866	\\
-0.648222895	7.259285388	\\
0.586609027	-7.492993083	\\
-0.59686477	2.706443959	\\
-1.195543992	8.107324517	\\
-0.193991052	0.867646956	\\
-0.404738406	1.781418354	\\
0.762604224	-5.861219415	\\
0.652921472	-3.30102624	\\
0.017070612	-5.692823752	\\
0.482210405	3.935216715	\\
-0.214499886	3.816405896	\\
1.975948536	1.462857269	\\
-0.462486025	7.378452187	\\
-0.485085705	6.138153526	\\
-0.791764101	6.426735698	\\
3.005168135	-25.74935543	\\
-0.007896082	4.172572968	\\
-0.381849788	2.173009992	\\
-0.575959993	2.753311554	\\
-0.339396558	1.433791834	\\
0.502293541	-0.252816843	\\
0.541082783	1.959733811	\\
0.294454826	-2.501804332	\\
1.033358627	-12.82736322	\\
0.623996705	-1.441999528	\\
-1.586901063	7.878096484	\\
0.015943354	0.010393494	\\
0.4400635	-0.127016546	\\
-0.15405971	1.648140779	\\
0.700525679	-10.76919359	\\
-0.311745963	3.171581805	\\
0.770154345	5.303428614	\\
0.778787458	-3.82867815	\\
0.767959538	-10.98557306	\\
-0.855854826	3.459252485	\\
-0.871030256	1.844936922	\\
-0.292380601	7.892199065	\\
-0.443631491	4.50007946	\\
-0.711329376	1.158236252	\\
0.647867771	0.863849128	\\
-0.976792137	1.424720775	\\
-1.000596518	1.888099452	\\
-1.113727892	2.495876259	\\
0.735430667	8.709640419	\\
0.747789681	-0.552989074	\\
1.277380053	2.752768024	\\
-0.065858729	2.456261845	\\
-0.478925336	3.521224917	\\
0.0260362	3.743047547	\\
0.087812322	3.198974138	\\
0.128587625	-2.763307603	\\
0.759762993	-4.215048408	\\
0.011874015	-3.311108786	\\
0.891100869	-7.746476341	\\
-1.194904164	8.601850696	\\
-1.154427133	4.702925474	\\
-0.752653754	2.14783217	\\
-0.580547927	-0.596960599	\\
-0.687346967	-16.27587901	\\
0.368888703	1.737508542	\\
0.188465834	1.197394961	\\
-0.74574211	1.77285954	\\
0.843159491	-5.175708499	\\
-0.137590633	-0.316505374	\\
0.231611014	8.45750006	\\
1.603730755	-4.14981132	\\
0.163414599	1.711343659	\\
-1.100687803	6.465927797	\\
2.07274014	-8.769892011	\\
-0.597201468	2.872822151	\\
1.353620033	0.487776406	\\
0.711126554	-4.151962877	\\
-0.887815938	2.301679341	\\
-0.129893133	4.072958596	\\
0.191612912	0.742030255	\\
-1.974900188	6.557156491	\\
-0.778325431	4.283751581	\\
-0.934648574	3.484482269	\\
-0.890578723	5.09364319	\\
0.430077026	-8.168208903	\\
-1.86921351	4.049604351	\\
0.342003915	-9.644827868	\\
0.704125418	-16.53278177	\\
-1.527467193	8.708349624	\\
0.04122769	0.261285824	\\
0.4539703	-1.007294847	\\
0.574996372	-1.883003264	\\
0.564963089	0.834329417	\\
-1.149435029	12.34448868	\\
2.561251034	-3.554022341	\\
1.838025373	-12.18248854	\\
-2.163132652	11.07511122	\\
0.973558634	-3.286280827	\\
0.301403147	-3.273446991	\\
-0.523281189	2.380275893	\\
-0.249487129	-4.336360337	\\
0.91489784	7.322227663	\\
-0.058493446	4.398131513	\\
0.293832655	-0.753180027	\\
-0.236242489	5.310890846	\\
-0.269321358	3.942616807	\\
-0.20446779	4.26771675	\\
-0.756951315	3.496083026	\\
-0.513605962	2.892845207	\\
0.061938126	2.27625206	\\
1.133058646	-7.73523262	\\
-0.447855154	4.173893517	\\
-1.046998616	3.338876596	\\
-0.748044301	-1.203319065	\\
1.164743285	-4.189072634	\\
-0.641913858	3.52712918	\\
0.785156449	5.478785222	\\
-0.083591058	0.744437883	\\
0.625590334	-2.777115787	\\
0.898690063	-3.725179197	\\
-1.151177611	4.366641361	\\
0.125535062	1.280178286	\\
0.353761583	-10.36912319	\\
-0.015801155	0.258887251	\\
-0.635947893	1.138677901	\\
-0.302031019	2.595878569	\\
0.424424931	4.628527887	\\
1.214520603	-0.240099997	\\
-0.114459343	-20.33933156	\\
-0.102106087	3.116264846	\\
-0.954810619	9.574396815	\\
-0.602720527	1.112158881	\\
0.871735438	-3.68264542	\\
-0.252036944	-0.082781299	\\
0.190174051	-21.95279465	\\
0.610815326	-0.356478406	\\
0.434173042	-1.015251806	\\
-0.287350285	7.838481558	\\
-0.69035129	7.76684516	\\
1.362458296	-0.85298447	\\
-1.405696031	5.181182397	\\
-1.250252105	5.60333901	\\
-0.262292105	-1.845942316	\\
0.841148577	4.547473408	\\
-0.406615331	-4.32310707	\\
-0.92413127	5.337518058	\\
-0.270590877	0.522511263	\\
-0.465662479	-2.729058225	\\
0.692506662	-5.201236377	\\
1.755988688	-5.654433456	\\
-1.554835023	4.524342086	\\
0.262317152	-4.930256877	\\
0.368433544	-0.094135634	\\
0.732142582	-2.44822043	\\
-0.144869443	3.724920611	\\
-1.241645053	5.630686429	\\
0.69738185	3.643993242	\\
0.278754817	0.182835635	\\
-1.872211784	12.11415014	\\
1.436641128	1.355995091	\\
2.706024264	-15.5143858	\\
-0.962461468	-8.969728594	\\
 };
  %\addplot[only marks, mark=*, red!80!black]table {\forbes};
  \addlegendentry{data}
 \addplot[no markers, very thick, red] table [row sep=\\,
 y={create col/linear regression={y=Y}}]{
 X Y \\
1.810684978	-11.50485012	\\
-1.393815488	9.381025966	\\
-0.655335721	2.997797302	\\
2.316355496	-18.65973441	\\
-0.100986622	3.461711494	\\
-0.180723623	4.289211563	\\
-1.303714187	7.281457834	\\
-0.115000798	-2.437709674	\\
0.731071923	-0.515556659	\\
-0.017617368	-0.056381809	\\
-0.505848218	0.496443205	\\
0.080327773	-1.251337644	\\
0.502328518	-2.981060484	\\
-1.817222998	7.753246102	\\
-0.493173914	2.35613315	\\
-1.167804934	3.149436525	\\
0.45264764	-0.644745189	\\
-0.115183012	5.782280021	\\
1.165947521	-4.319947457	\\
0.841834694	-4.911766024	\\
-1.023986957	1.760450414	\\
-0.074341086	1.493285706	\\
-1.662684507	6.392526534	\\
0.132071676	-23.19075468	\\
-0.45560242	1.023135391	\\
0.419217955	-1.33820223	\\
-1.032587372	3.347299621	\\
1.617888788	-4.178843831	\\
-0.750410681	-3.201691362	\\
-1.444804744	1.753828024	\\
0.677193996	-11.86625933	\\
0.343673092	-0.248232963	\\
-0.717539912	7.310109168	\\
0.381300663	-0.97931426	\\
0.202695142	-9.185025206	\\
1.813377302	-11.27630082	\\
-0.359235176	5.442162062	\\
-1.255055961	3.743126995	\\
-0.299607516	4.030775373	\\
0.826170843	0.230620086	\\
1.323572413	-10.07336065	\\
0.356265708	-0.797492866	\\
-0.648222895	7.259285388	\\
0.586609027	-7.492993083	\\
-0.59686477	2.706443959	\\
-1.195543992	8.107324517	\\
-0.193991052	0.867646956	\\
-0.404738406	1.781418354	\\
0.762604224	-5.861219415	\\
0.652921472	-3.30102624	\\
0.017070612	-5.692823752	\\
0.482210405	3.935216715	\\
-0.214499886	3.816405896	\\
1.975948536	1.462857269	\\
-0.462486025	7.378452187	\\
-0.485085705	6.138153526	\\
-0.791764101	6.426735698	\\
3.005168135	-25.74935543	\\
-0.007896082	4.172572968	\\
-0.381849788	2.173009992	\\
-0.575959993	2.753311554	\\
-0.339396558	1.433791834	\\
0.502293541	-0.252816843	\\
0.541082783	1.959733811	\\
0.294454826	-2.501804332	\\
1.033358627	-12.82736322	\\
0.623996705	-1.441999528	\\
-1.586901063	7.878096484	\\
0.015943354	0.010393494	\\
0.4400635	-0.127016546	\\
-0.15405971	1.648140779	\\
0.700525679	-10.76919359	\\
-0.311745963	3.171581805	\\
0.770154345	5.303428614	\\
0.778787458	-3.82867815	\\
0.767959538	-10.98557306	\\
-0.855854826	3.459252485	\\
-0.871030256	1.844936922	\\
-0.292380601	7.892199065	\\
-0.443631491	4.50007946	\\
-0.711329376	1.158236252	\\
0.647867771	0.863849128	\\
-0.976792137	1.424720775	\\
-1.000596518	1.888099452	\\
-1.113727892	2.495876259	\\
0.735430667	8.709640419	\\
0.747789681	-0.552989074	\\
1.277380053	2.752768024	\\
-0.065858729	2.456261845	\\
-0.478925336	3.521224917	\\
0.0260362	3.743047547	\\
0.087812322	3.198974138	\\
0.128587625	-2.763307603	\\
0.759762993	-4.215048408	\\
0.011874015	-3.311108786	\\
0.891100869	-7.746476341	\\
-1.194904164	8.601850696	\\
-1.154427133	4.702925474	\\
-0.752653754	2.14783217	\\
-0.580547927	-0.596960599	\\
-0.687346967	-16.27587901	\\
0.368888703	1.737508542	\\
0.188465834	1.197394961	\\
-0.74574211	1.77285954	\\
0.843159491	-5.175708499	\\
-0.137590633	-0.316505374	\\
0.231611014	8.45750006	\\
1.603730755	-4.14981132	\\
0.163414599	1.711343659	\\
-1.100687803	6.465927797	\\
2.07274014	-8.769892011	\\
-0.597201468	2.872822151	\\
1.353620033	0.487776406	\\
0.711126554	-4.151962877	\\
-0.887815938	2.301679341	\\
-0.129893133	4.072958596	\\
0.191612912	0.742030255	\\
-1.974900188	6.557156491	\\
-0.778325431	4.283751581	\\
-0.934648574	3.484482269	\\
-0.890578723	5.09364319	\\
0.430077026	-8.168208903	\\
-1.86921351	4.049604351	\\
0.342003915	-9.644827868	\\
0.704125418	-16.53278177	\\
-1.527467193	8.708349624	\\
0.04122769	0.261285824	\\
0.4539703	-1.007294847	\\
0.574996372	-1.883003264	\\
0.564963089	0.834329417	\\
-1.149435029	12.34448868	\\
2.561251034	-3.554022341	\\
1.838025373	-12.18248854	\\
-2.163132652	11.07511122	\\
0.973558634	-3.286280827	\\
0.301403147	-3.273446991	\\
-0.523281189	2.380275893	\\
-0.249487129	-4.336360337	\\
0.91489784	7.322227663	\\
-0.058493446	4.398131513	\\
0.293832655	-0.753180027	\\
-0.236242489	5.310890846	\\
-0.269321358	3.942616807	\\
-0.20446779	4.26771675	\\
-0.756951315	3.496083026	\\
-0.513605962	2.892845207	\\
0.061938126	2.27625206	\\
1.133058646	-7.73523262	\\
-0.447855154	4.173893517	\\
-1.046998616	3.338876596	\\
-0.748044301	-1.203319065	\\
1.164743285	-4.189072634	\\
-0.641913858	3.52712918	\\
0.785156449	5.478785222	\\
-0.083591058	0.744437883	\\
0.625590334	-2.777115787	\\
0.898690063	-3.725179197	\\
-1.151177611	4.366641361	\\
0.125535062	1.280178286	\\
0.353761583	-10.36912319	\\
-0.015801155	0.258887251	\\
-0.635947893	1.138677901	\\
-0.302031019	2.595878569	\\
0.424424931	4.628527887	\\
1.214520603	-0.240099997	\\
-0.114459343	-20.33933156	\\
-0.102106087	3.116264846	\\
-0.954810619	9.574396815	\\
-0.602720527	1.112158881	\\
0.871735438	-3.68264542	\\
-0.252036944	-0.082781299	\\
0.190174051	-21.95279465	\\
0.610815326	-0.356478406	\\
0.434173042	-1.015251806	\\
-0.287350285	7.838481558	\\
-0.69035129	7.76684516	\\
1.362458296	-0.85298447	\\
-1.405696031	5.181182397	\\
-1.250252105	5.60333901	\\
-0.262292105	-1.845942316	\\
0.841148577	4.547473408	\\
-0.406615331	-4.32310707	\\
-0.92413127	5.337518058	\\
-0.270590877	0.522511263	\\
-0.465662479	-2.729058225	\\
0.692506662	-5.201236377	\\
1.755988688	-5.654433456	\\
-1.554835023	4.524342086	\\
0.262317152	-4.930256877	\\
0.368433544	-0.094135634	\\
0.732142582	-2.44822043	\\
-0.144869443	3.724920611	\\
-1.241645053	5.630686429	\\
0.69738185	3.643993242	\\
0.278754817	0.182835635	\\
-1.872211784	12.11415014	\\
1.436641128	1.355995091	\\
2.706024264	-15.5143858	\\
-0.962461468	-8.969728594	\\
 }; 
\addlegendentry{%
    $\pgfmathprintnumber{\pgfplotstableregressionb}
    \pgfmathprintnumber[print sign]{\pgfplotstableregressiona} \cdot x$} %
\end{axis}
\end{tikzpicture}
\end{center}
\end{frame}
\begin{frame}[fragile=singleslide]{R result}
\begin{Verbatim}[frame=single,label=R code,
fontseries=b,xleftmargin=2mm,commandchars=\\\{\},
formatcom=\color{blue}]
Call:
lm(formula = lifeExpF ~ log(ppgdp) + fertility)

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  63.4484     3.7446  16.944  < 2e-16 ***
log(ppgdp)    2.4150     0.3386   7.132 1.86e-11 ***
fertility    -4.1991     0.3938 -10.664  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 5.142 on 196 degrees of freedom
Multiple R-squared:  0.7446,	Adjusted R-squared:  0.742 
\end{Verbatim}
\end{frame}

\begin{frame}[fragile=singleslide]{R result of $e_{y}=\gamma_{0}+\gamma_{1}e_{x2}$}
\begin{Verbatim}[frame=single,label=R code,
fontseries=b,xleftmargin=2mm,commandchars=\\\{\},
formatcom=\color{blue}]
> me<-lm(Y ~ X); summary(me)

Call:
lm(formula = Y ~ X)
Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.681e-16  3.636e-01    0.00        1    
X           -4.199e+00  3.928e-01  -10.69   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 5.129 on 197 degrees of freedom
Multiple R-squared:  0.3672,	Adjusted R-squared:  0.364 
\end{Verbatim}
\end{frame}
\begin{frame}{分析結果}
\begin{itemize}

\item 當沒有$X_{1}$時，$\hat{\beta_{2}}=-6.22$
\item 當去掉$X_{1}$時，$\hat{\beta_{2}}=-4.19$
\item 因此，考慮$X_{1}$的解釋變異量之後，$X_{2}$減少了33\%。
\item $R^2=0.367$，表示考慮$X_{1}$的解釋變異量之後，增加$X_{2}$可以多解釋沒有被解釋的36.7\%變異量。
\end{itemize}
\end{frame}

\begin{frame}[fragile=singleslide]{R result of $Y=\beta_{0}+\beta_{1}e_{x2}$}
\begin{Verbatim}[frame=single,label=R code,
fontseries=b,xleftmargin=2mm,commandchars=\\\{\},
formatcom=\color{blue}]
m1<- lm(fertility ~ log(ppgdp))
Call:
lm(formula = lifeExpF ~ m1＄residuals)

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)   72.2932     0.6640 108.871  < 2e-16 ***
m1＄residuals  -4.1991     0.7172  -5.855 1.97e-08 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 9.367 on 197 degrees of freedom
Multiple R-squared:  0.1482,	Adjusted R-squared:  0.1439 
\end{Verbatim}
\end{frame}

\begin{frame}{結果分析}
\begin{itemize}
\item 分別用$X_{1}$以及考慮$X_{1}$之後的$e_{x2}$解釋$Y$，分別得到$R^2$為0.59以及0.14。
\item 兩者相加剛好是用$X_{1}$以及$X_{2}$解釋$Y$的變異量。
\item 因此，$e_{x2}$代表跟$X_{1}$無關的$X_{2}$的部分。
\end{itemize}
\end{frame}
\section{結論}
\begin{frame}\frametitle{\H 總結}
\begin{enumerate}
\item {\K 瞭解何謂複迴歸}
\item {\K 瞭解一個連續變數、一個類別變數的模型}
\item {\K 瞭解兩個連續變數的估計}
\item {\K 瞭解變異數分析}
\end{enumerate}
\end{frame}

\end{document}
